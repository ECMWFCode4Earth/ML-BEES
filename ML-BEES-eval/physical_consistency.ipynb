{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a few checks of physical consistency.\n",
    "\n",
    "First check whether XGB and MLP react to changes in field capacity in terms of average soil moisture. For this, the inference has to be re-done with modified inputs.\n",
    "\n",
    "The second part of this notebook involves the water balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from eval_utilities import spatial_temporal_metrics as stm\n",
    "from eval_utilities import visualization as vis\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(f\"config.yaml\") as stream:\n",
    "    try:\n",
    "        CONFIG = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "ds_ref = xr.open_zarr(CONFIG[\"path_ec_euro\"]).sel(time=slice(\"2021-01-01T00\", \"2022-11-30T00\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soil Moisture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field Capacity vs. Average Soil Moisture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether we see the expected relationship between grid points with different field capacities in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mod = xr.open_zarr(CONFIG[\"inf_paths\"][\"xgb_diag_v2\"]).sel(time=slice(\"2021-01-01T00\", \"2022-11-30T00\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Climate info:\n",
    "field_cap = ds_ref.clim_data.sel(clim_variable=\"clim_theta_cap\")\n",
    "wilt_point = ds_ref.clim_data.sel(clim_variable=\"clim_theta_pwp\")\n",
    "\n",
    "# Average over time and soil moisture levels:\n",
    "avg_ref = ds_ref.sel(variable=[\"swvl1\",\"swvl2\",\"swvl3\"]).data.mean(dim=(\"time\", \"variable\"))\n",
    "avg_mod = ds_mod.sel(variable=[\"swvl1\",\"swvl2\",\"swvl3\"]).data.mean(dim=(\"time\", \"variable\"))\n",
    "\n",
    "\n",
    "# Assemble data for boxplots:\n",
    "fcaps = sorted(set(field_cap.values))\n",
    "boxes_ref = []\n",
    "boxes_mod = []\n",
    "for fcap in fcaps:\n",
    "    _ = avg_ref.where(ds_ref.clim_data.sel(clim_variable=\"clim_theta_cap\") == fcap).values\n",
    "    _ = _[~np.isnan(_)]\n",
    "    boxes_ref.append(_)\n",
    "\n",
    "    _ = avg_mod.where(ds_ref.clim_data.sel(clim_variable=\"clim_theta_cap\") == fcap).values\n",
    "    _ = _[~np.isnan(_)]\n",
    "    boxes_mod.append(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr = plt.boxplot(boxes_ref, positions=np.array(range(len(fcaps)))*2.0-0.4, widths=0.6)\n",
    "bpm = plt.boxplot(boxes_mod, positions=np.array(range(len(fcaps)))*2.0+0.4, widths=0.6)\n",
    "plt.xticks(range(0, len(fcaps) * 2, 2), fcaps)\n",
    "\n",
    "# Distinguish mod and ref:\n",
    "plt.setp(bpr['medians'], color=\"tab:orange\", label=\"ref\")\n",
    "plt.setp(bpm['medians'], color=\"tab:blue\", label=\"mod\")\n",
    "\n",
    "# Get rid of duplicates in the legend:\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys())\n",
    "\n",
    "plt.xlabel(\"Field Capacity\")\n",
    "plt.ylabel(\"Average (time and vert) Soil Moisture\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity of Soil Moisture to Soil Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the static inputs are altered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first part is modified from \"inference_xgboost.iypnb\". It contains all necessary ingredients for running the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "\n",
    "from eval_utilities.EclandPointDataset import EcDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = CONFIG[\"path_ec_euro\"]\n",
    "model_path = \"/home/ch23/weights_ch23/euro_xgb_train_2019_val_2020_all_variables.json\"\n",
    "spatial_encoding = False\n",
    "temporal_encoding = False\n",
    "\n",
    "# Dataset:\n",
    "ds_inf = EcDataset(start_year = 2020, end_year = 2022, root = data_path, roll_out = 1, \n",
    "                   #clim_features=CONFIG[\"clim_feats\"], dynamic_features=CONFIG[\"dynamic_feats\"],\n",
    "                   #target_prog_features=CONFIG[\"targets_prog\"], target_diag_features=CONFIG[\"targets_diag\"],\n",
    "                   is_add_lat_lon = spatial_encoding, \n",
    "                   is_norm = True, \n",
    "                   point_dropout = 0.0)\n",
    "\n",
    "# XGB Model:\n",
    "model = xgb.XGBRegressor(n_estimators=1000, tree_method=\"hist\", device=\"cuda\")\n",
    "model.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should neither be stored nor is it necessary to predict the whole data set. Instead, we want to run the same grid point with different parameters.\n",
    "Here, we are modifying only field capacitiy (for now). To get the whole range, field capacity is varied from $0$ to $0.75$ in steps of $0.05$.\n",
    "The test sites are repeated and stacked to run the inference for all variations at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose test sites:\n",
    "x_coords = [63699, 62986, 62267, 85271] #Bonn, Juelich, Reading, Bologna\n",
    "ii = np.searchsorted(ds_ref[\"x\"], x_coords) #corresponding indices\n",
    "\n",
    "# Get index of field capacity in clim input:\n",
    "i_cap = np.where(np.array(CONFIG[\"clim_feats\"]) == \"clim_theta_cap\")[0][0]\n",
    "\n",
    "# Define new field capacities and normalize:\n",
    "new_caps = (np.arange(0, 0.8, 0.05) - ds_inf.clim_means[i_cap]) / ds_inf.clim_stdevs[i_cap]\n",
    "\n",
    "\n",
    "# Initial state\n",
    "_, x_state, _, _, x_clim, _ = ds_inf[0]\n",
    "x_state, x_clim = x_state.squeeze()[ii], x_clim.squeeze()[ii]\n",
    "preds = [EcDataset.inv_transform(x_state, ds_inf.y_prog_means, ds_inf.y_prog_stdevs)]\n",
    "\n",
    "\n",
    "# Change initial state to contain duplicates of the test sites with the new field capacities:\n",
    "new_states = []\n",
    "new_clims = []\n",
    "for i_site in range(len(x_coords)):\n",
    "    new_states.append(np.vstack([x_state[i_site]] * len(new_caps)))\n",
    "    new_clims.append(np.vstack([x_clim[i_site]] * len(new_caps)))\n",
    "    new_clims[-1][:,i_cap] = new_caps\n",
    "\n",
    "x_state = np.vstack(new_states)\n",
    "x_clim = np.vstack(new_clims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained to be applied to each time step:\n",
    "def apply_physical_constraints(x_state):\n",
    "    x_state[:, np.array(ds_inf.target_prog_features)!=\"e\"] = np.clip(x_state[:, np.array(ds_inf.target_prog_features)!=\"e\"], 0, None) # All variables except \"e\" are positive\n",
    "    x_state[:,-1] = np.clip(x_state[:,-1], None, 100) # Snow cover cannot be higher than 100\n",
    "    return x_state\n",
    "\n",
    "# Inference\n",
    "for i in tqdm(range(len(ds_inf)), desc=\"Running ECLand emulator...\"):\n",
    "    x_met, _, _, _, _, _ = ds_inf[i]\n",
    "    x_met = x_met.squeeze()[ii]\n",
    "    \n",
    "    # Duplicate weather conditions to match the structure above:\n",
    "    x_met = np.vstack([np.vstack([x_met[i_site]] * len(new_caps)) for i_site in range(len(x_coords))])\n",
    "    \n",
    "    X = np.concatenate((x_met, x_state, x_clim), axis=1)\n",
    "    y_pred = model.predict(X)\n",
    "    y_state_inc_pred = y_pred[:,:len(ds_inf.target_prog_features)]\n",
    "    y_state_inc_pred = EcDataset.inv_transform(y_state_inc_pred, ds_inf.y_prog_inc_mean, ds_inf.y_prog_inc_std) # Unnormalize so that it can be added to the normalized state vector\n",
    "    x_state += y_state_inc_pred\n",
    "    x_state = apply_physical_constraints(EcDataset.inv_transform(x_state, ds_inf.y_prog_means, ds_inf.y_prog_stdevs)) # Unnormalize updated state vector and apply consistency constraints\n",
    "    preds.append(x_state)\n",
    "    x_state = EcDataset.transform(x_state, ds_inf.y_prog_means, ds_inf.y_prog_stdevs) # Re-normalize state vector for next iteration\n",
    "\n",
    "result = np.stack(preds[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "i_swvl1 = np.where(np.array(ds_inf.target_prog_features) == \"swvl1\")[0][0]\n",
    "\n",
    "for i in range(12):\n",
    "    ax.plot(ds_inf.times[1:], result[:,i,i_swvl1])\n",
    "\n",
    "ax.set(ylabel=\"swvl1\")\n",
    "plt.tick_params(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from eval_utilities.model.MLP import MLP\n",
    "from eval_utilities.EclandPointDataset import EcDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ch23/weights_ch23/last/mlp/config.yaml') as stream:\n",
    "    try:\n",
    "        MODEL_CONFIG = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "data_path = CONFIG[\"path_ec_euro\"]\n",
    "model_path = \"/home/ch23/weights_ch23/last/mlp/model_checkpoints/best_loss_model.pth\"\n",
    "\n",
    "spatial_encoding = True\n",
    "temporal_encoding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "ds_inf = EcDataset(\n",
    "    start_year = 2020,\n",
    "    end_year = 2022,\n",
    "    root = data_path,\n",
    "    roll_out = 1,\n",
    "    clim_features=MODEL_CONFIG[\"clim_feats\"],\n",
    "    dynamic_features=MODEL_CONFIG[\"dynamic_feats\"],\n",
    "    target_prog_features=MODEL_CONFIG[\"targets_prog\"],\n",
    "    target_diag_features=MODEL_CONFIG[\"targets_diag\"],\n",
    "    is_add_lat_lon = spatial_encoding,\n",
    "    is_norm = True,\n",
    "    point_dropout = 0.0\n",
    ")\n",
    "\n",
    "# MLP Model\n",
    "model = MLP(in_static=ds_inf.n_static,\n",
    "    in_dynamic=ds_inf.n_dynamic,\n",
    "    in_prog=ds_inf.n_prog,\n",
    "    out_prog=ds_inf.n_prog,\n",
    "    out_diag=ds_inf.n_diag,\n",
    "    hidden_dim=MODEL_CONFIG[\"hidden_dim\"],\n",
    "    rollout=MODEL_CONFIG[\"roll_out\"],\n",
    "    dropout=MODEL_CONFIG[\"dropout\"],\n",
    "    mu_norm=ds_inf.y_prog_inc_mean,\n",
    "    std_norm=ds_inf.y_prog_inc_std,\n",
    "    pretrained=model_path\n",
    ")#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose test sites:\n",
    "x_coords = [63699, 62986, 62267, 85271, 28081] #Bonn, Juelich, Reading, Bologna, Uimaniemi (partly lake GP in Finland)\n",
    "ii = np.searchsorted(ds_ref[\"x\"], x_coords) #corresponding indices\n",
    "\n",
    "# Get index of field capacity in clim input:\n",
    "i_cap = np.where(np.array(MODEL_CONFIG[\"clim_feats\"]) == \"clim_theta_cap\")[0][0]\n",
    "\n",
    "# Define new field capacities and normalize:\n",
    "new_caps = (np.arange(0, 0.8, 0.05) - ds_inf.clim_means[i_cap]) / ds_inf.clim_stdevs[i_cap]\n",
    "\n",
    "\n",
    "# Initial state\n",
    "_, x_state, _, _, x_clim, _ = ds_inf[0]\n",
    "x_state, x_clim = x_state.squeeze()[ii], x_clim.squeeze()[ii]\n",
    "preds = [EcDataset.inv_transform(x_state, ds_inf.y_prog_means, ds_inf.y_prog_stdevs)]\n",
    "\n",
    "\n",
    "# Change initial state to contain duplicates of the test sites with the new field capacities:\n",
    "new_states = []\n",
    "new_clims = []\n",
    "for i_site in range(len(x_coords)):\n",
    "    new_states.append(np.vstack([x_state[i_site]] * len(new_caps)))\n",
    "    new_clims.append(np.vstack([x_clim[i_site]] * len(new_caps)))\n",
    "    new_clims[-1][:,i_cap] = new_caps\n",
    "\n",
    "x_state = np.vstack(new_states)\n",
    "x_clim = torch.from_numpy(np.vstack(new_clims)[np.newaxis, np.newaxis, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to apply to each model step\n",
    "def apply_constraints_prog(x):\n",
    "    x = np.clip(x, 0, None) # All prog. variables are positive\n",
    "    x[:,np.array(CONFIG[\"targets_prog\"]) == \"snowc\"] = np.clip(x[:,np.array(CONFIG[\"targets_prog\"]) == \"snowc\"], None, 100) # Snow cover cannot be higher than 100\n",
    "    return x\n",
    "\n",
    "def apply_constraints_diag(x):\n",
    "    for i in range(x.shape[1]):\n",
    "        if CONFIG[\"targets_diag\"][i] not in [\"e\", \"slhf\", \"sshf\"]:\n",
    "            x[:,i] = np.clip(x[:,i], 0, None)\n",
    "    # x[:,np.array(CONFIG[\"targets_diag\"]) not in [\"slhf\", \"sshf\", \"e\"]] = np.clip(x[:,np.array(CONFIG[\"targets_diag\"]) not in [\"slhf\", \"sshf\", \"e\"]], 0, None) # All variables except e are positive\n",
    "    return x\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Initial state\n",
    "    prognostic_preds = []\n",
    "    #_, x_state, _, _, x_clim, _ = ds_inf[0]\n",
    "    ## x_state, x_clim = torch.from_numpy(x_state).unsqueeze(1).cuda(), torch.from_numpy(x_clim).unsqueeze(1).cuda()\n",
    "    #x_state, x_clim = x_state.squeeze(), torch.from_numpy(x_clim)[:, None, :, :]\n",
    "    prognostic_preds.append(EcDataset.inv_transform(x_state, ds_inf.y_prog_means, ds_inf.y_prog_stdevs))\n",
    "    diagnostic_preds = []\n",
    "\n",
    "    # Inference\n",
    "    for i in tqdm(range(len(ds_inf)), desc=\"Running ECLand emulator...\"):\n",
    "        x_met, _, _, _, _, x_time = ds_inf[i]\n",
    "        # x_met, x_time = torch.from_numpy(x_met).unsqueeze(1).cuda(), torch.from_numpy(x_time).unsqueeze(1).cuda()\n",
    "        x_met, x_state, x_time = torch.from_numpy(x_met)[:, None, :, :], torch.from_numpy(x_state)[None, None, :, :], torch.from_numpy(x_time)[:, None, :]\n",
    "        \n",
    "        # Cut out sites from whole field:\n",
    "        x_met = x_met.squeeze()[ii]\n",
    "    \n",
    "        # Duplicate weather conditions to match the structure above:\n",
    "        x_met = np.vstack([np.vstack([x_met[i_site]] * len(new_caps)) for i_site in range(len(x_coords))])\n",
    "        x_met = torch.from_numpy(x_met[np.newaxis, np.newaxis, :, :])\n",
    "        \n",
    "        \n",
    "        y_state_inc_pred, y_diag_pred, _, _ = model(x_clim, x_met, x_state, x_time)\n",
    "        y_state_inc_pred, y_diag_pred, x_state = y_state_inc_pred.detach().numpy().squeeze(), y_diag_pred.detach().numpy().squeeze(), x_state.numpy().squeeze()\n",
    "        # Prognostic variables\n",
    "        y_state_inc_pred = EcDataset.inv_transform(y_state_inc_pred, ds_inf.y_prog_inc_mean, ds_inf.y_prog_inc_std) # Unnormalize so that it can be added to the normalized state vector\n",
    "        x_state += y_state_inc_pred\n",
    "        x_state = apply_constraints_prog(EcDataset.inv_transform(x_state, ds_inf.y_prog_means, ds_inf.y_prog_stdevs)) # Unnormalize updated state vector and apply consistency constraints\n",
    "        prognostic_preds.append(x_state)\n",
    "        x_state = EcDataset.transform(x_state, ds_inf.y_prog_means, ds_inf.y_prog_stdevs) # Re-normalize state vector for next iteration\n",
    "        # Diagnostic variables\n",
    "        y_diag_pred = apply_constraints_diag(EcDataset.inv_transform(y_diag_pred, ds_inf.y_diag_means, ds_inf.y_diag_stdevs))\n",
    "        diagnostic_preds.append(y_diag_pred)\n",
    "\n",
    "# Diagnostic variables for the last timestep are not part of the dataset, so we add a \"dummy\"\n",
    "diagnostic_preds.append(y_diag_pred)\n",
    "\n",
    "all_preds = np.concatenate((np.stack(prognostic_preds), np.stack(diagnostic_preds)), axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relation: Avg Soil Moisture lvl 1 to field capaciity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "i_swvl1 = np.where(np.array(ds_inf.target_prog_features) == \"swvl1\")[0][0]\n",
    "\n",
    "for s, site in enumerate([\"Bonn\", \"Juelich\", \"Reading\", \"Bologna\", \"Uimaniemi\"]):\n",
    "    swvl1_avgs = [np.mean(all_preds[:, s * len(new_caps) + i, i_swvl1]) for i in range(len(new_caps))]\n",
    "    ax.scatter(np.arange(0, 0.8, 0.05), swvl1_avgs, marker=\".\", label=site)\n",
    "\n",
    "ax.set(title=\"Sensitivity of MLP to field capacity\", xlabel=\"Field Capacity\", ylabel=\"Average lvl 1 Soil Moisture\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to compare the sensitivity w.r.t. field capacity in terms of error metrics. To do this comparison, the inference above was run with only the original field capacity values and saved in a pickle file. These inferences are loaded in the next cell and then measured against the reference data set.\n",
    "\n",
    "For re-creating the inference file, go to the cell for setting up the test sites and change:\\\n",
    "`new_caps = ...` to `new_caps = [1]`\\\n",
    "and comment out this line `new_clims[-1][:,i_cap] = new_caps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"preds_original_clim.pckl\",'rb') as f:\n",
    "    preds_orig_clim = pickle.load(f)\n",
    "\n",
    "# Clunky workaround for using the eval_utils functions:\n",
    "orig_xr = xr.DataArray(\n",
    "            data = preds_orig_clim[:,:,i_swvl1:i_swvl1+1],\n",
    "            coords = {\"x\": x_coords, \"time\": ds_inf.times, \"variable\": [\"swvl1\"]},\n",
    "            dims = [\"time\", \"x\", \"variable\"],\n",
    "            name = \"data\"\n",
    "        ).to_dataset()\n",
    "\n",
    "# Compute bias and rmse of unchanged clim data to ECLand:\n",
    "orig_bias = stm.bias(orig_xr, ds_ref, \"swvl1\")\n",
    "orig_rmse = stm.rmse(orig_xr, ds_ref, \"swvl1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, site in enumerate([\"Bonn\", \"Juelich\", \"Reading\", \"Bologna\", \"Uimaniemi\"]):\n",
    "    # Get the original field capacity data:\n",
    "    clim_cap = ds_ref.sel(x=x_coords[s]).clim_data.sel(clim_variable=\"clim_theta_cap\").values\n",
    "\n",
    "    # Loop over all modified values for a single site:\n",
    "    res_bias = np.full(len(new_caps), np.nan)\n",
    "    res_rmse = np.full(len(new_caps), np.nan)\n",
    "    \n",
    "    for i in range(len(new_caps)):\n",
    "        # Clunky workaround for using the eval_utils functions:\n",
    "        pred_xr = xr.DataArray(\n",
    "            data = all_preds[:, s * len(new_caps) + i: s * len(new_caps) + i+1 , i_swvl1:i_swvl1+1],\n",
    "            coords = {\"x\": [x_coords[s]], \"time\": ds_inf.times, \"variable\": [\"swvl1\"]},\n",
    "            dims = [\"time\", \"x\", \"variable\"],\n",
    "            name = \"data\"\n",
    "        ).to_dataset()\n",
    "\n",
    "        res_bias[i] = stm.bias(pred_xr, ds_ref, \"swvl1\")\n",
    "        res_rmse[i] = stm.rmse(pred_xr, ds_ref, \"swvl1\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(np.arange(0, 0.8, 0.05), res_bias, \".-\", color=\"tab:blue\", label=\"Bias\")\n",
    "    ax.scatter(clim_cap, orig_bias.sel(x=x_coords[s]), color=\"tab:blue\")\n",
    "\n",
    "    ax.plot(np.arange(0, 0.8, 0.05), res_rmse, \".-\", color=\"tab:orange\", label=\"RMSE\")\n",
    "    ax.scatter(clim_cap, orig_rmse.sel(x=x_coords[s]), color=\"tab:orange\")\n",
    "    \n",
    "    ax.set(title=f\"Sensitivity of MLP to field capacity at {site} grid point\", xlabel=\"Field Capacity\", ylabel=\"Metric Value for swvl1\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compared to SMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAP\n",
    "ds = xr.open_zarr(\"/data/ch23/data_ch23/smap_sm_interp.zarr\")\n",
    "ds_smap = xr.DataArray(\n",
    "    data = ds[\"smap_sm\"].data[:,:,np.newaxis],\n",
    "    coords = {\"x\":ds[\"x\"], \"time\":ds[\"time\"], \"variable\":[\"swvl1\"]},\n",
    "    dims = [\"time\", \"x\", \"variable\"],\n",
    "    name = \"data\"\n",
    ")\n",
    "ds_smap = ds_smap.assign_coords(lon=(\"x\", ds_ref[\"lon\"].data))\n",
    "ds_smap = ds_smap.assign_coords(lat=(\"x\", ds_ref[\"lat\"].data))\n",
    "ds_smap = ds_smap.to_dataset()\n",
    "ds_smap = ds_smap.dropna(dim=\"time\", how=\"all\")\n",
    "\n",
    "orig_bias = stm.bias(orig_xr, ds_smap, \"swvl1\")\n",
    "orig_rmse = stm.rmse(orig_xr, ds_smap, \"swvl1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, site in enumerate([\"Bonn\", \"Juelich\", \"Reading\", \"Bologna\", \"Uimaniemi\"]):\n",
    "    # Get the original field capacity data:\n",
    "    clim_cap = ds_ref.sel(x=x_coords[s]).clim_data.sel(clim_variable=\"clim_theta_cap\").values\n",
    "\n",
    "    # Loop over all modified values for a single site:\n",
    "    res_bias = np.full(len(new_caps), np.nan)\n",
    "    res_rmse = np.full(len(new_caps), np.nan)\n",
    "    \n",
    "    for i in range(len(new_caps)):\n",
    "        # Clunky workaround for using the eval_utils functions:\n",
    "        pred_xr = xr.DataArray(\n",
    "            data = all_preds[:, s * len(new_caps) + i: s * len(new_caps) + i+1 , i_swvl1:i_swvl1+1],\n",
    "            coords = {\"x\": [x_coords[s]], \"time\": ds_inf.times, \"variable\": [\"swvl1\"]},\n",
    "            dims = [\"time\", \"x\", \"variable\"],\n",
    "            name = \"data\"\n",
    "        ).to_dataset()\n",
    "\n",
    "        res_bias[i] = stm.bias(pred_xr, ds_smap, \"swvl1\")\n",
    "        res_rmse[i] = stm.rmse(pred_xr, ds_smap, \"swvl1\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(np.arange(0, 0.8, 0.05), res_bias, \".-\", color=\"tab:blue\", label=\"Bias (vs. SMAP)\")\n",
    "    ax.scatter(clim_cap, orig_bias.sel(x=x_coords[s]), color=\"tab:blue\")\n",
    "\n",
    "    ax.plot(np.arange(0, 0.8, 0.05), res_rmse, \".-\", color=\"tab:orange\", label=\"RMSE (vs. SMAP)\")\n",
    "    ax.scatter(clim_cap, orig_rmse.sel(x=x_coords[s]), color=\"tab:orange\")\n",
    "    \n",
    "    ax.set(title=f\"Sensitivity of MLP to field capacity at {site} grid point\", xlabel=\"Field Capacity\", ylabel=\"Metric Value for swvl1\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity of Evaporation to Soil Moisture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_coords = {\"Bonn\": 63699, \"Juelich\": 62986, \"Reading\": 62267, \"Bologna\": 85271}\n",
    "\n",
    "mod_lhf = ds_mod.sel(variable=\"slhf\").data / 3600.\n",
    "ref_lhf = ds_ref.sel(variable=\"slhf\").data / 3600.\n",
    "R_n = ds_ref.sel(variable=\"met_lwdown\").data + ds_ref.sel(variable=\"met_swdown\").data\n",
    "\n",
    "mod_sm = ds_mod.sel(variable=[\"swvl1\",\"swvl2\"]).data.mean(dim=\"variable\")\n",
    "ref_sm = ds_ref.sel(variable=[\"swvl1\",\"swvl2\"]).data.mean(dim=\"variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,x in x_coords.items():\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.scatter(mod_sm.sel(x=x), -mod_lhf.sel(x=x) / R_n.sel(x=x), marker=\".\")\n",
    "    ax.scatter(ref_sm.sel(x=x), -ref_lhf.sel(x=x) / R_n.sel(x=x), marker=\".\")\n",
    "\n",
    "    ax.axvline(wilt_point.sel(x=x), color=\"tab:grey\")\n",
    "    ax.text(wilt_point.sel(x=x), 0.99, 'WILT', color='tab:grey', ha='right', va='top', rotation=90, transform=ax.get_xaxis_transform())\n",
    "\n",
    "    ax.set(title=f\"SM Regimes for {name}\", xlabel=\"Average Soil Moisture lvl 1-2\", ylabel=\"Latent Flux / Incoming Radiation\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the water balance equation: delta S=P-E-R -- long-term delta S should be 0 -- yearly?\n",
    "eval_ds = xr.open_zarr(\"/data/ch23/data_ch23/euro_xgb_train_2010_2019_val_2020_2020_diagnostic_v2.zarr\")  # Global dataset\n",
    "# train_ds = xr.open_zarr(\"/data/ecland_i6aj_o400_2010_2022_6h_euro.zarr\")  # Europe subset\n",
    "eval_ds"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting \n",
    "(eval_ds\n",
    " .data\n",
    " .isel(time=2)\n",
    " .sel(variable=\"e\").to_dataset()\n",
    " .plot.scatter(x=\"lon\", y=\"lat\", hue=\"data\", s=10, edgecolors=\"none\",figsize=(12,6))\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 9,
>>>>>>> baabe33 (finalize yk's scripts)
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = xr.open_zarr(\"/data/ecland_i6aj_o400_2010_2022_6h_euro.zarr\").sel(time=slice(\"2020\", \"2021\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with hourly data in 2017 2018\n",
    "\n",
    "train_ds = xr.open_zarr(\"/data/ecland_i6aj_o400_2017_2018_1h_euro.zarr\").sel(time=slice(\"2017\", \"2018\"))  "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 10,
>>>>>>> baabe33 (finalize yk's scripts)
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yearly Water Balance Evaluation...Test in 2020\n",
    "# load the water balance equation components\n",
    "# (P+snow fall)-ET-R=delta S\n",
    "\n",
    "start_date = '2020-01-01 00:00:00'\n",
    "end_date = '2020-12-31 18:00:00'\n",
    "train_ds_e= train_ds.data.sel(variable=\"e\",time=slice(start_date, end_date))\n",
    "train_ds_sro= train_ds.data.sel(variable=\"sro\",time=slice(start_date, end_date))\n",
    "train_ds_ssro= train_ds.data.sel(variable=\"ssro\",time=slice(start_date, end_date))\n",
    "train_ds_swvl1= train_ds.data.sel(variable=\"swvl1\",time=slice(start_date, end_date))\n",
    "train_ds_swvl2= train_ds.data.sel(variable=\"swvl2\",time=slice(start_date, end_date))\n",
    "train_ds_swvl3= train_ds.data.sel(variable=\"swvl3\",time=slice(start_date, end_date))\n",
    "train_ds_snow= train_ds.data.sel(variable=\"met_snowf\",time=slice(start_date, end_date))\n",
    "\n",
    "eval_ds_e= eval_ds.data.sel(variable=\"e\",time=slice(start_date, end_date))\n",
    "eval_ds_sro= eval_ds.data.sel(variable=\"sro\",time=slice(start_date, end_date)) # averaged over the grid area\n",
    "eval_ds_ssro= eval_ds.data.sel(variable=\"ssro\",time=slice(start_date, end_date))\n",
    "eval_ds_swvl1= eval_ds.data.sel(variable=\"swvl1\",time=slice(start_date, end_date))\n",
    "eval_ds_swvl2= eval_ds.data.sel(variable=\"swvl2\",time=slice(start_date, end_date))\n",
    "eval_ds_swvl3= eval_ds.data.sel(variable=\"swvl3\",time=slice(start_date, end_date))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_snow= train_ds.data.sel(variable=\"met_snowf\",time=slice(start_date, end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_ds_snow.values[:,120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 11,
>>>>>>> baabe33 (finalize yk's scripts)
   "metadata": {},
   "outputs": [],
   "source": [
    "# runoff == sur + subsur -- unit in m -- acuumulated\n",
    "# using 00:00 everyday as the accumulated daily total amount\n",
    "\n",
    "train_ds_runoff=train_ds_sro+train_ds_ssro\n",
    "eval_ds_runoff=eval_ds_sro+eval_ds_ssro\n",
    "\n",
    "train_acc_runoff = train_ds_runoff.sel(time=train_ds_runoff['time'].dt.hour == 0).sum(dim='time')\n",
    "eval_acc_runoff = eval_ds_runoff.sel(time=eval_ds_runoff['time'].dt.hour == 0).sum(dim='time')\n",
    "\n",
    "train_acc_e = train_ds_e.sel(time=train_ds_e['time'].dt.hour == 0).sum(dim='time')\n",
    "eval_acc_e = eval_ds_e.sel(time=eval_ds_e['time'].dt.hour == 0).sum(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_snow = train_ds_snow.sum(dim='time')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_ds_runoff.values[:6,120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 12,
>>>>>>> baabe33 (finalize yk's scripts)
   "metadata": {},
   "outputs": [],
   "source": [
    "# p unit transfer -- original mm/s to m/hour\n",
    "# 6h p data -- then mm/s to m/6hour -- mutiply by 3.6*6\n",
    "\n",
    "train_ds_p = train_ds.data.sel(variable=\"met_rainf\",time=slice(start_date, end_date))\n",
    "train_ds_p_acc = train_ds_p.sum(dim='time')*3.6*6 # in total amount of P in meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total water storage in 3 soil layers\n",
    "# area (m2) *depth=total volumne of each soil layer --> * swv ratio = total amount of soil water (m3)\n",
    "train_total_volume_swvl1=train_ds.clim_data.sel(clim_variable=\"clim_cell_area\")*0.07*train_ds_swvl1\n",
    "train_total_volume_swvl2=train_ds.clim_data.sel(clim_variable=\"clim_cell_area\")*0.21*train_ds_swvl2\n",
    "train_total_volume_swvl3=train_ds.clim_data.sel(clim_variable=\"clim_cell_area\")*0.72*train_ds_swvl3\n",
    "train_total_volume_swvl=train_total_volume_swvl1+train_total_volume_swvl2+train_total_volume_swvl3\n",
    "train_grid_mean_swvl=train_total_volume_swvl/train_ds.clim_data.sel(clim_variable=\"clim_cell_area\") # convert to unit in depth in m\n",
    "\n",
    "eval_total_volume_swvl1=train_ds.clim_data.sel(clim_variable=\"clim_cell_area\")*0.07*eval_ds_swvl1\n",
    "eval_total_volume_swvl2=train_ds.clim_data.sel(clim_variable=\"clim_cell_area\")*0.21*eval_ds_swvl2\n",
    "eval_total_volume_swvl3=train_ds.clim_data.sel(clim_variable=\"clim_cell_area\")*0.72*eval_ds_swvl3\n",
    "eval_total_volume_swvl=eval_total_volume_swvl1+eval_total_volume_swvl2+eval_total_volume_swvl3\n",
    "eval_grid_mean_swvl=eval_total_volume_swvl/train_ds.clim_data.sel(clim_variable=\"clim_cell_area\") # unit in meter\n",
    "\n",
    "#changes in soil water: end of year - beginning of year\n",
    "changes_in_swvl_train=train_grid_mean_swvl.isel(time=0)-train_grid_mean_swvl.isel(time=-1)\n",
    "changes_in_swvl_eval=eval_grid_mean_swvl.isel(time=0)-eval_grid_mean_swvl.isel(time=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHanges in soil water at each year = YearSum(P) - YearSum(E)- YearSum(R); unit in m\n",
    "# https://www.geo.fu-berlin.de/en/v/iwm-network/learning_content/environmental-background/basics_hydrogeography/water_balance/index.html\n",
    "# http://mena-rainwater.org/water-balance/\n",
    "diff=changes_in_swvl_train-(train_ds_p_acc.variable.values+train_acc_snow.variable.values+train_acc_e-train_acc_runoff)\n",
    "diff_eval=changes_in_swvl_eval-(train_ds_p_acc.variable.values+train_acc_snow.variable.values+eval_acc_e-eval_acc_runoff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the output\n",
    "\n",
    "lat = changes_in_swvl_train['lat'].values\n",
    "lon = changes_in_swvl_train['lon'].values\n",
    "values = changes_in_swvl_train.variable.values\n",
    "\n",
    "# Create the plot\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sc=plt.scatter(lon, lat, c=values, cmap='viridis', s=10,vmin=-0.15,vmax=0.15)\n",
    "cbar = plt.colorbar(sc, ax=ax)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "cbar.set_label(\"SWC/m\", fontsize=16)  # Set the label for the colorbar\n",
    "#cbar = plt.colorbar(sc, label='E (m)', fontsize=16)  # Increase label font size\n",
    "\n",
    "plt.xlabel('Longitude', fontsize=16)\n",
    "plt.ylabel('Latitude', fontsize=16)\n",
    "plt.title('Annual total changes in SWC 2020', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the output\n",
    "\n",
    "lat = train_ds_p_acc['lat'].values\n",
    "lon = train_ds_p_acc['lon'].values\n",
    "values = train_ds_p_acc.variable.values\n",
    "\n",
    "# Create the plot\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sc=plt.scatter(lon, lat, c=values, cmap='viridis', s=10,vmin=0,vmax=1.2)\n",
    "cbar = plt.colorbar(sc, ax=ax)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "cbar.set_label(\"P/m\", fontsize=16)  # Set the label for the colorbar\n",
    "#cbar = plt.colorbar(sc, label='E (m)', fontsize=16)  # Increase label font size\n",
    "\n",
    "plt.xlabel('Longitude', fontsize=16)\n",
    "plt.ylabel('Latitude', fontsize=16)\n",
    "plt.title('Annual total P 2020', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 51,
>>>>>>> baabe33 (finalize yk's scripts)
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the output -- p+snow\n",
    "\n",
    "lat = train_ds_p_acc['lat'].values\n",
    "lon = train_ds_p_acc['lon'].values\n",
    "values = train_ds_p_acc.variable.values+train_acc_snow.variable.values\n",
    "\n",
    "# Create the plot\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sc=plt.scatter(lon, lat, c=values, cmap='viridis', s=10,vmin=0.2,vmax=2)\n",
    "cbar = plt.colorbar(sc, ax=ax)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "cbar.set_label(\"P/m\", fontsize=16)  # Set the label for the colorbar\n",
    "#cbar = plt.colorbar(sc, label='E (m)', fontsize=16)  # Increase label font size\n",
    "\n",
    "plt.xlabel('Longitude', fontsize=16)\n",
    "plt.ylabel('Latitude', fontsize=16)\n",
    "plt.title('Annual total P+snow fall 2020', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the output -- Annual E \n",
    "lat = train_acc_e['lat'].values\n",
    "lon = train_acc_e['lon'].values\n",
    "values = train_acc_e.variable.values*(-1)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sc=plt.scatter(lon, lat, c=values, cmap='viridis', s=10, vmin=0, vmax=1.2)\n",
    "cbar = plt.colorbar(sc, ax=ax)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "cbar.set_label(\"E/m\", fontsize=16) # Set the label for the colorbar\n",
    "#cbar = plt.colorbar(sc, label='E (m)', fontsize=16)  # Increase label font size\n",
    "\n",
    "plt.xlabel('Longitude', fontsize=16)\n",
    "plt.ylabel('Latitude', fontsize=16)\n",
    "plt.title('Annual total E 2020', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the output -- Annual SRO+SSRO\n",
    "\n",
    "lat = train_acc_runoff['lat'].values\n",
    "lon = train_acc_runoff['lon'].values\n",
    "values = train_acc_runoff.variable.values\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sc=plt.scatter(lon, lat, c=values, cmap='viridis', s=10,vmin=0,vmax=3)\n",
    "cbar = plt.colorbar(sc, ax=ax)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "cbar.set_label(\"R/m\", fontsize=16)  # Set the label for the colorbar\n",
    "#cbar = plt.colorbar(sc, label='E (m)', fontsize=16)  # Increase label font size\n",
    "\n",
    "plt.xlabel('Longitude', fontsize=16)\n",
    "plt.ylabel('Latitude', fontsize=16)\n",
    "plt.title('Annual total runoff 2020', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the output\n",
    "\n",
    "lat = diff['lat'].values\n",
    "lon = diff['lon'].values\n",
    "values = train_ds_p_acc.variable.values+train_acc_snow.variable.values-train_acc_e.variable.values*(-1)-train_acc_runoff.variable.values\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sc=plt.scatter(lon, lat, c=values, cmap='viridis', s=10,vmin=-0.2,vmax=0.2)\n",
    "cbar = plt.colorbar(sc, ax=ax)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "cbar.set_label(\"diff/m\", fontsize=16)  # Set the label for the colorbar\n",
    "#cbar = plt.colorbar(sc, label='E (m)', fontsize=16)  # Increase label font size\n",
    "\n",
    "plt.xlabel('Longitude', fontsize=16)\n",
    "plt.ylabel('Latitude', fontsize=16)\n",
    "plt.title('(P+snow-E-R) 2020', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 68,
>>>>>>> baabe33 (finalize yk's scripts)
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the water balance equation of ECLand model in 2020\n",
    "\n",
    "lat = diff['lat'].values\n",
    "lon = diff['lon'].values\n",
    "values = train_ds_p_acc.variable.values+train_acc_snow.variable.values-train_acc_e.variable.values*(-1)-train_acc_runoff.variable.values\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sc=plt.scatter(lon, lat, c=values, cmap='viridis', s=10,vmin=-0.2,vmax=0.2)\n",
    "cbar = plt.colorbar(sc, ax=ax)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "cbar.set_label(\"diff/m\", fontsize=16)  # Set the label for the colorbar\n",
    "#cbar = plt.colorbar(sc, label='E (m)', fontsize=16)  # Increase label font size\n",
    "\n",
    "plt.xlabel('Longitude', fontsize=16)\n",
    "plt.ylabel('Latitude', fontsize=16)\n",
    "plt.title('ECLand: P+snow-E-R 2020', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the water balance equation of XGBoost model in 2020\n",
    "\n",
    "lat = diff['lat'].values\n",
    "lon = diff['lon'].values\n",
    "values = train_ds_p_acc.variable.values+train_acc_snow.variable.values-eval_acc_e.variable.values*(-1)-eval_acc_runoff.variable.values\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sc=plt.scatter(lon, lat, c=values, cmap='viridis', s=10,vmin=-0.2,vmax=0.2)\n",
    "cbar = plt.colorbar(sc, ax=ax)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "cbar.set_label(\"diff/m\", fontsize=16)  # Set the label for the colorbar\n",
    "#cbar = plt.colorbar(sc, label='E (m)', fontsize=16)  # Increase label font size\n",
    "\n",
    "plt.xlabel('Longitude', fontsize=16)\n",
    "plt.ylabel('Latitude', fontsize=16)\n",
    "plt.title('XGBoost: P+snow-E-R 2020', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the difference term in water balance equation in ECLand\n",
    "\n",
    "lat = diff['lat'].values\n",
    "lon = diff['lon'].values\n",
    "values = diff.variable.values\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sc=plt.scatter(lon, lat, c=values, cmap='viridis', s=10,vmin=-0.2,vmax=0.2)\n",
    "cbar = plt.colorbar(sc, ax=ax)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "cbar.set_label(\"diff/m\", fontsize=16)  # Set the label for the colorbar\n",
    "#cbar = plt.colorbar(sc, label='E (m)', fontsize=16)  # Increase label font size\n",
    "\n",
    "plt.xlabel('Longitude', fontsize=16)\n",
    "plt.ylabel('Latitude', fontsize=16)\n",
    "plt.title('ECLand: changes in SWC-(P-E-R) 2020', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the difference term in water balance equation in XGBoost\n",
    "\n",
    "lat = diff['lat'].values\n",
    "lon = diff['lon'].values\n",
    "values = diff_eval.variable.values\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sc=plt.scatter(lon, lat, c=values, cmap='viridis', s=10,vmin=-0.2,vmax=0.2)\n",
    "cbar = plt.colorbar(sc, ax=ax)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "cbar.set_label(\"diff/m\", fontsize=16)  # Set the label for the colorbar\n",
    "#cbar = plt.colorbar(sc, label='E (m)', fontsize=16)  # Increase label font size\n",
    "\n",
    "plt.xlabel('Longitude', fontsize=16)\n",
    "plt.ylabel('Latitude', fontsize=16)\n",
    "plt.title('XGBoost: changes in SWC-(P-E-R) 2020', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.show()"
   ]
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a daily balance\n",
    "# Yearly Water Balance Evaluation...Test in 2020\n",
    "# P-ET-R=delta S\n",
    "# what is the unit? \n",
    "# P seems to be much smaller than E?\n",
    "# Discharge is very large\n",
    "start_date = '2020-08-01 00:00:00'\n",
    "end_date = '2020-08-02 00:00:00'\n",
    "train_ds_e= train_ds.data.sel(variable=\"e\",time=slice(start_date, end_date))\n",
    "train_ds_sro= train_ds.data.sel(variable=\"sro\",time=slice(start_date, end_date))\n",
    "train_ds_ssro= train_ds.data.sel(variable=\"ssro\",time=slice(start_date, end_date))\n",
    "train_ds_swvl1= train_ds.data.sel(variable=\"swvl1\",time=slice(start_date, end_date))\n",
    "train_ds_swvl2= train_ds.data.sel(variable=\"swvl2\",time=slice(start_date, end_date))\n",
    "train_ds_swvl3= train_ds.data.sel(variable=\"swvl3\",time=slice(start_date, end_date))\n",
    "\n",
    "eval_ds_e= eval_ds.data.sel(variable=\"e\",time=slice(start_date, end_date))\n",
    "eval_ds_sro= eval_ds.data.sel(variable=\"sro\",time=slice(start_date, end_date)) # averaged over the grid area\n",
    "eval_ds_ssro= eval_ds.data.sel(variable=\"ssro\",time=slice(start_date, end_date))\n",
    "eval_ds_swvl1= eval_ds.data.sel(variable=\"swvl1\",time=slice(start_date, end_date))\n",
    "eval_ds_swvl2= eval_ds.data.sel(variable=\"swvl2\",time=slice(start_date, end_date))\n",
    "eval_ds_swvl3= eval_ds.data.sel(variable=\"swvl3\",time=slice(start_date, end_date))\n",
    "# convert to day mean?\n",
    "#changes in soil water: end of year - beginning of year\n",
    "# total water storage in 3 soil layers\n",
    "# area (m2) *depth=total volumne of each soil layer --> * swv ratio = total amount of soil water (m3)\n",
    "train_total_volume_swvl1=train_ds.clim_data.sel(clim_variable=\"clim_cell_area\")*0.07*train_ds_swvl1\n",
    "train_total_volume_swvl2=train_ds.clim_data.sel(clim_variable=\"clim_cell_area\")*0.21*train_ds_swvl2\n",
    "train_total_volume_swvl3=train_ds.clim_data.sel(clim_variable=\"clim_cell_area\")*0.72*train_ds_swvl3\n",
    "train_total_volume_swvl=train_total_volume_swvl1+train_total_volume_swvl2+train_total_volume_swvl3\n",
    "train_grid_mean_swvl=train_total_volume_swvl/train_ds.clim_data.sel(clim_variable=\"clim_cell_area\") # convert to unit in depth in m\n",
    "\n",
    "eval_total_volume_swvl1=train_ds.clim_data.sel(clim_variable=\"clim_cell_area\")*0.07*eval_ds_swvl1\n",
    "eval_total_volume_swvl2=train_ds.clim_data.sel(clim_variable=\"clim_cell_area\")*0.21*eval_ds_swvl2\n",
    "eval_total_volume_swvl3=train_ds.clim_data.sel(clim_variable=\"clim_cell_area\")*0.72*eval_ds_swvl3\n",
    "eval_total_volume_swvl=eval_total_volume_swvl1+eval_total_volume_swvl2+eval_total_volume_swvl3\n",
    "eval_grid_mean_swvl=eval_total_volume_swvl/train_ds.clim_data.sel(clim_variable=\"clim_cell_area\") # unit in meter\n",
    "\n",
    "# convert to day mean?\n",
    "#changes in soil water: end of year - beginning of year\n",
    "changes_in_swvl_train=train_grid_mean_swvl.isel(time=0)-train_grid_mean_swvl.isel(time=-1)\n",
    "changes_in_swvl_eval=eval_grid_mean_swvl.isel(time=0)-eval_grid_mean_swvl.isel(time=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runoff == sur + subsur -- unit in m -- acuumulated\n",
    "train_ds_runoff=train_ds_sro+train_ds_ssro\n",
    "eval_ds_runoff=eval_ds_sro+eval_ds_ssro\n",
    "\n",
    "\n",
    "# time calculation in daily mean? but using which time step? -- using 00:00 everyday as the accumulated daily total amount\n",
    "train_acc_runoff = train_ds_runoff.sel(time=train_ds_runoff['time'].dt.hour == 0).sum(dim='time')\n",
    "eval_acc_runoff = eval_ds_runoff.sel(time=eval_ds_runoff['time'].dt.hour == 0).sum(dim='time')\n",
    "\n",
    "train_acc_e = train_ds_e.sel(time=train_ds_e['time'].dt.hour == 0).sum(dim='time')\n",
    "eval_acc_e = eval_ds_e.sel(time=eval_ds_e['time'].dt.hour == 0).sum(dim='time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p unit transfer\n",
    "\n",
    "train_ds_p = train_ds.data.sel(variable=\"met_rainf\",time=slice(start_date, end_date))\n",
    "train_ds_p_acc = train_ds_p.sum(dim='time') # in m\n",
    "# daily sum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff=changes_in_swvl_train-(train_ds_p_acc*10+train_acc_e-train_acc_runoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the output\n",
    "\n",
    "lat = changes_in_swvl_train['lat'].values\n",
    "lon = changes_in_swvl_train['lon'].values\n",
    "values = changes_in_swvl_train.variable.values\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(lon, lat, c=values, cmap='viridis', s=10,vmin=0,vmax=0.02)\n",
    "plt.colorbar(label='swvl (m)')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Changes in soil water 2020-01-01')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the output\n",
    "\n",
    "lat = train_ds_p_acc['lat'].values\n",
    "lon = train_ds_p_acc['lon'].values\n",
    "values = train_ds_p_acc.variable.values*10\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(lon, lat, c=values, cmap='viridis', s=10,vmin=0,vmax=0.02)\n",
    "plt.colorbar(label='P(m)')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Annual total P 2020-01-01')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the output -- Annual E \n",
    "\n",
    "lat = train_acc_e['lat'].values\n",
    "lon = train_acc_e['lon'].values\n",
    "values = train_acc_e.variable.values*(-1)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(lon, lat, c=values, cmap='viridis', s=10,vmin=0,vmax=0.02)\n",
    "plt.colorbar(label='E (m)')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Annual total E 2020-01-01')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the output -- Annual SRO+SSRO\n",
    "\n",
    "lat = train_acc_runoff['lat'].values\n",
    "lon = train_acc_runoff['lon'].values\n",
    "values = train_acc_runoff.variable.values\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(lon, lat, c=values, cmap='viridis', s=10,vmin=0,vmax=0.02)\n",
    "plt.colorbar(label='Runoff (m)')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Annual total runoff 2020-08-01')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the output -- Annual SRO+SSRO\n",
    "\n",
    "lat = train_acc_runoff['lat'].values\n",
    "lon = train_acc_runoff['lon'].values\n",
    "values = train_ds_p_acc.variable.values*10-train_acc_e.variable.values*(-1)-train_acc_runoff.variable.values\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(lon, lat, c=values, cmap='viridis', s=10,vmin=0,vmax=0.02)\n",
    "plt.colorbar(label='Runoff (m)')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Annual total runoff 2020-08-01')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the output 01-01\n",
    "\n",
    "lat = diff['lat'].values\n",
    "lon = diff['lon'].values\n",
    "values = diff.variable.values\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(lon, lat, c=values, cmap='viridis', s=10)\n",
    "plt.colorbar(label='diff(m)')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Spatial Plot of Climate Variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the output -08-01\n",
    "\n",
    "lat = diff['lat'].values\n",
    "lon = diff['lon'].values\n",
    "values = diff.variable.values\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(lon, lat, c=values, cmap='viridis', s=10,vmin=-0.02,vmax=0.02)\n",
    "plt.colorbar(label='diff(m)')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Spatial Plot of Climate Variable')\n",
    "plt.show()"
   ]
=======
>>>>>>> baabe33 (finalize yk's scripts)
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
